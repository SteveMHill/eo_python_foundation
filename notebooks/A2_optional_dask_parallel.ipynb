{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Course header](../assets/img/header.png)\n",
    "\n",
    "# A2 â€” Parallel Computing with Dask\n",
    "Optional reference â€” scale your EO workflows beyond laptop RAM\n",
    "\n",
    "This appendix introduces **Dask**, a library that adds chunked, parallel execution to NumPy and xarray.\n",
    "You will create a local cluster, compute NDVI in parallel, and learn the lazy-evaluation pattern.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "- Explain why Dask is needed for large-scale EO analysis\n",
    "- Create a local Dask cluster and open the Dashboard\n",
    "- Open raster data with `chunks=` for lazy, parallel processing\n",
    "- Understand the difference between lazy graphs and `.compute()`\n",
    "- Follow best practices for chunk sizing\n",
    "\n",
    "Tooling:\n",
    "- dask.distributed\n",
    "- xarray + rioxarray\n",
    "- matplotlib\n",
    "\n",
    "â±ï¸ Estimated time: **30 â€“ 45 minutes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. Why Dask?\n",
    "2. Setup â€” create a local cluster\n",
    "3. Lazy evaluation with xarray + Dask\n",
    "4. Parallel NDVI example\n",
    "5. Best practices\n",
    "6. Scaling beyond your laptop\n",
    "7. Cleanup\n",
    "8. Exercises\n",
    "9. Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1) Why Dask?\n",
    "\n",
    "A single Sentinel-2 tile at 10 m resolution is ~1 GB.  \n",
    "A year of data over a country can easily exceed **hundreds of GB**.\n",
    "\n",
    "| Challenge | Daskâ€™s solution |\n",
    "|-----------|------------------|\n",
    "| Data doesnâ€™t fit in RAM | **Chunking** â€” process piece by piece |\n",
    "| Loading data you donâ€™t need | **Lazy evaluation** â€” load on demand |\n",
    "| Single-core bottleneck | **Parallelisation** â€” use all cores |\n",
    "\n",
    "Dask integrates seamlessly with xarray:  \n",
    "open data with `chunks=` and every subsequent xarray operation runs in parallel automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2) Setup â€” create a local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(\n",
    "    n_workers=4,\n",
    "    threads_per_worker=1,\n",
    "    memory_limit='2GB',\n",
    ")\n",
    "client = Client(cluster)\n",
    "client  # click the Dashboard link to monitor workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dask Dashboard\n",
    "\n",
    "Click the **Dashboard link** above. It shows:\n",
    "\n",
    "- **Task Stream** â€” what each worker is doing over time\n",
    "- **Progress** â€” overall bar for the current computation\n",
    "- **Memory** â€” RAM usage per worker\n",
    "\n",
    "> ðŸ’¡ **Tip:** In JupyterLab the Dask sidebar (orange icon) lets you embed dashboard panels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3) Lazy evaluation with xarray + Dask\n",
    "\n",
    "When you open data with `chunks=`, xarray stores **Dask arrays** instead of NumPy arrays.  \n",
    "Operations build a task graph but **donâ€™t execute** until you call `.compute()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a COG with chunking enabled â€” nothing is loaded yet\n",
    "cog_url = (\n",
    "    'https://sentinel-cogs.s3.us-west-2.amazonaws.com/'\n",
    "    'sentinel-s2-l2a-cogs/32/T/PS/2024/12/'\n",
    "    'S2B_32TPS_20241228_0_L2A/B04.tif'\n",
    ")\n",
    "\n",
    "red = rioxarray.open_rasterio(cog_url, chunks='auto')\n",
    "\n",
    "print(f'Shape:  {red.shape}')\n",
    "print(f'Chunks: {red.chunks}')\n",
    "print(f'Backed by: {type(red.data).__name__}')  # dask.array\n",
    "red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain lazy operations â€” NO data is loaded\n",
    "red_sq = red.squeeze('band', drop=True)\n",
    "patch  = red_sq.isel(x=slice(2000, 4000), y=slice(2000, 4000))\n",
    "refl   = patch / 10_000.0\n",
    "mean_r = refl.mean()\n",
    "\n",
    "print(f'Mean (lazy): {mean_r}')  # still a Dask scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triggering computation: `.compute()`\n",
    "\n",
    "Call `.compute()` to execute the graph.  Watch the Dashboard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = mean_r.compute()\n",
    "print(f'Mean reflectance: {result:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4) Parallel NDVI example\n",
    "\n",
    "Load Red and NIR bands with chunking, compute NDVI, and plot â€” all in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = (\n",
    "    'https://sentinel-cogs.s3.us-west-2.amazonaws.com/'\n",
    "    'sentinel-s2-l2a-cogs/32/T/PS/2024/12/'\n",
    "    'S2B_32TPS_20241228_0_L2A'\n",
    ")\n",
    "\n",
    "red = rioxarray.open_rasterio(\n",
    "    f'{base_url}/B04.tif', chunks={'x': 1024, 'y': 1024}\n",
    ").squeeze('band', drop=True)\n",
    "\n",
    "nir = rioxarray.open_rasterio(\n",
    "    f'{base_url}/B08.tif', chunks={'x': 1024, 'y': 1024}\n",
    ").squeeze('band', drop=True)\n",
    "\n",
    "print(f'Red chunks: {red.chunks}')\n",
    "print(f'NIR chunks: {nir.chunks}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDVI â€” still lazy\n",
    "ndvi = (nir - red) / (nir + red)\n",
    "ndvi.name = 'ndvi'\n",
    "\n",
    "print(f'Backed by: {type(ndvi.data).__name__}')\n",
    "print(f'Chunks:    {len(ndvi.chunks[0])} Ã— {len(ndvi.chunks[1])}')\n",
    "ndvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Coarsen for faster demo, then compute\n",
    "ndvi_small = ndvi.coarsen(x=4, y=4, boundary='trim').mean()\n",
    "ndvi_computed = ndvi_small.compute()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ndvi_computed.plot(ax=ax, cmap='RdYlGn', vmin=-0.5, vmax=0.8)\n",
    "ax.set_title('NDVI (computed with Dask)')\n",
    "ax.set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5) Best practices\n",
    "\n",
    "### When to call `.compute()`\n",
    "\n",
    "| Situation | Approach |\n",
    "|-----------|----------|\n",
    "| Building a processing chain | Stay **lazy** â€” chain operations |\n",
    "| Checking an intermediate value | `.compute()` on a small subset |\n",
    "| Final result | `.compute()` |\n",
    "| Saving to file | `.to_netcdf()` / `.to_zarr()` trigger compute |\n",
    "| Plotting | `.plot()` triggers compute |\n",
    "\n",
    "### Chunk-size guidelines\n",
    "\n",
    "- Target **100 MB â€“ 1 GB** per chunk (rule of thumb)\n",
    "- Too small â†’ scheduling overhead\n",
    "- Too large â†’ memory pressure\n",
    "- Align chunks with COG tile size or Zarr chunk layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check actual chunk size in MB\n",
    "chunk_mb = (\n",
    "    red.data.chunksize[0] * red.data.chunksize[1]\n",
    "    * red.dtype.itemsize / 1e6\n",
    ")\n",
    "print(f'Chunk size: {chunk_mb:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common mistakes\n",
    "\n",
    "| Mistake | Why itâ€™s bad |\n",
    "|---------|-------------|\n",
    "| Calling `.values` on a huge array | Loads everything into memory |\n",
    "| Rechunking repeatedly | Expensive shuffle |\n",
    "| Millions of tiny tasks | Scheduler overhead |\n",
    "| Forgetting to close the client | Resource leak |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6) Scaling beyond your laptop\n",
    "\n",
    "The same code works on bigger systems â€” only the cluster definition changes:\n",
    "\n",
    "| Environment | Cluster type | Package |\n",
    "|-------------|--------------|----------|\n",
    "| Laptop | `LocalCluster` | dask.distributed |\n",
    "| HPC (SLURM / PBS) | `SLURMCluster` | dask-jobqueue |\n",
    "| Kubernetes | `KubeCluster` | dask-kubernetes |\n",
    "| Managed cloud | `Gateway` | Dask Gateway / Coiled |\n",
    "\n",
    "```python\n",
    "# Example: SLURM cluster on an HPC\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "cluster = SLURMCluster(cores=4, memory='16GB', walltime='01:00:00')\n",
    "cluster.scale(jobs=10)\n",
    "client = Client(cluster)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7) Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()\n",
    "print('âœ… Dask cluster closed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8) Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Try it â€” Chunk comparison\n",
    "\n",
    "1. Re-open the Red band COG with `chunks={'x': 512, 'y': 512}`.\n",
    "2. Print the number of chunks (`len(red.chunks[0]) * len(red.chunks[1])`).\n",
    "3. Compare with `chunks={'x': 2048, 'y': 2048}` â€” how many chunks now?\n",
    "4. Which would you expect to be faster? Why?\n",
    "\n",
    "<details><summary>Show solution</summary>\n",
    "\n",
    "```python\n",
    "red_512 = rioxarray.open_rasterio(\n",
    "    f\"{base_url}/B04.tif\", chunks={\"x\": 512, \"y\": 512}\n",
    ").squeeze(\"band\", drop=True)\n",
    "n_512 = len(red_512.chunks[0]) * len(red_512.chunks[1])\n",
    "\n",
    "red_2048 = rioxarray.open_rasterio(\n",
    "    f\"{base_url}/B04.tif\", chunks={\"x\": 2048, \"y\": 2048}\n",
    ").squeeze(\"band\", drop=True)\n",
    "n_2048 = len(red_2048.chunks[0]) * len(red_2048.chunks[1])\n",
    "\n",
    "print(f\"512Ã—512  â†’ {n_512:,} chunks\")\n",
    "print(f\"2048Ã—2048 â†’ {n_2048:,} chunks\")\n",
    "print(\"Fewer, larger chunks usually run faster (less scheduling overhead).\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Open red band with 512 and 2048 chunk sizes â€” compare chunk counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Try it â€” Parallel statistics\n",
    "\n",
    "Compute the **min, max, and standard deviation** of the NDVI array (before coarsening). Use `.compute()` once on all three.\n",
    "\n",
    "<details><summary>Show solution</summary>\n",
    "\n",
    "```python\n",
    "ndvi_stats = xr.Dataset({\n",
    "    \"min\": ndvi.min(),\n",
    "    \"max\": ndvi.max(),\n",
    "    \"std\": ndvi.std(),\n",
    "}).compute()\n",
    "\n",
    "print(ndvi_stats)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Compute min, max, and std of ndvi in one .compute() call\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§  Checkpoint\n",
    "\n",
    "**Q1.** What does `chunks='auto'` do when opening a raster?\n",
    "\n",
    "- A) Loads the entire file into a NumPy array\n",
    "- B) Lets Dask choose chunk sizes that match the file's internal tiling\n",
    "- C) Disables chunking\n",
    "\n",
    "**Q2.** When does a Dask computation actually run?\n",
    "\n",
    "- A) As soon as you write the expression\n",
    "- B) When you call `.compute()` (or plot / save)\n",
    "- C) It never runs automatically\n",
    "\n",
    "**Q3.** What is a good target chunk size?\n",
    "\n",
    "- A) 1 KB\n",
    "- B) 100 MB â€“ 1 GB\n",
    "- C) 50 GB\n",
    "\n",
    "<details><summary>Show answers</summary>\n",
    "\n",
    "1. **B** â€” `chunks='auto'` lets Dask pick chunk sizes aligned with the file's internal tiling (COG block size or Zarr chunks).\n",
    "2. **B** â€” Dask is lazy; computation happens only when you call `.compute()`, `.values`, `.plot()`, or save to a file.\n",
    "3. **B** â€” Target 100 MB â€“ 1 GB per chunk. Too small = scheduling overhead; too large = memory pressure.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9) Recap\n",
    "\n",
    "| Concept | Key takeaway |\n",
    "|---------|-------------|\n",
    "| Dask | Chunked parallel execution for NumPy / xarray |\n",
    "| `LocalCluster` | Run workers on your own machine |\n",
    "| `chunks=` | Turns xarray arrays into lazy Dask graphs |\n",
    "| `.compute()` | Triggers actual execution |\n",
    "| Dashboard | Monitor workers, tasks, and memory |\n",
    "\n",
    "ðŸ’¡ **Stay lazy as long as possible** â€” chain operations before computing.\n",
    "\n",
    "### Further reading\n",
    "\n",
    "- [Dask documentation](https://docs.dask.org/)\n",
    "- [xarray with Dask](https://docs.xarray.dev/en/stable/user-guide/dask.html)\n",
    "- [Dask best practices](https://docs.dask.org/en/stable/best-practices.html)\n",
    "- [Pangeo](https://pangeo.io/) â€” community using Dask for geoscience"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
